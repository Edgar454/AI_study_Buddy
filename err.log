/root/.local/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:623: UserWarning: <built-in function callable> is not a Python type (it may be an instance of an object), Pydantic will allow any object with no validation since we cannot even enforce that the input is an instance of the given type. To get rid of this error wrap the type with `pydantic.SkipValidation`.
  warn(
/root/.local/lib/python3.11/site-packages/passlib/utils/__init__.py:854: DeprecationWarning: 'crypt' is deprecated and slated for removal in Python 3.13
  from crypt import crypt as _crypt
/root/.local/lib/python3.11/site-packages/celery/platforms.py:841: SecurityWarning: You're running the worker with superuser privileges: this is
absolutely not recommended!

Please specify a different user using the --uid option.

User information: uid=0 euid=0 gid=0 egid=0

  warnings.warn(SecurityWarning(ROOT_DISCOURAGED.format(
[2025-05-07 07:10:33,826: INFO/MainProcess] Connected to redis://redis:6379/0
[2025-05-07 07:10:33,833: INFO/MainProcess] mingle: searching for neighbors
[2025-05-07 07:10:33,851: WARNING/MainProcess] /root/.local/lib/python3.11/site-packages/kombu/transport/redis.py:514: DeprecationWarning: Call to 'get_connection' function with deprecated usage of input argument/s '['command_name']'. (Use get_connection() without args instead) -- Deprecated since version 5.3.0.
  client.connection = client.connection_pool.get_connection('_')

[2025-05-07 07:10:34,875: INFO/MainProcess] mingle: all alone
[2025-05-07 07:10:34,920: INFO/MainProcess] celery@4a69253a7ca5 ready.
[2025-05-07 07:12:48,688: INFO/MainProcess] Task src.celery_app.process_file_task[2d3f3fec-eef4-47e7-a286-b2ef5a9250b5] received
[2025-05-07 07:12:49,975: WARNING/ForkPoolWorker-1] File 91f6d0a5091461ccc94795fc13a85427f56959850db35e898bee4e06e0286d59 not found in cache. Processing...
[2025-05-07 07:12:50,799: WARNING/ForkPoolWorker-1] 🖇 AgentOps: [34mSession Replay: https://app.agentops.ai/sessions?trace_id=97ef5a5a7215205ba6f5451ab9c4450e[0m
[2025-05-07 07:12:50,806: WARNING/ForkPoolWorker-1] ╭─────────────────────────── Crew Execution Started ───────────────────────────╮
│                                                                              │
│  Crew Execution Started                                                      │
│  Name: crew                                                                  │
│  ID: 3eddbcf1-77c4-4060-8e9d-bb0cf6a7f9d5                                    │
│                                                                              │
│                                                                              │
╰──────────────────────────────────────────────────────────────────────────────╯
[2025-05-07 07:12:52,498: WARNING/ForkPoolWorker-1] 🚀 Crew: crew
└── 📋 Task: d06e3dbe-1412-4822-a3bb-133b7215da38
       Status: Executing Task...
[2025-05-07 07:12:52,505: WARNING/ForkPoolWorker-1] 🚀 Crew: crew
└── 📋 Task: d06e3dbe-1412-4822-a3bb-133b7215da38
       Status: Executing Task...
    └── 🤖 Agent: Specialiste de la recherche
        
            Status: In Progress
[2025-05-07 07:12:52,506: WARNING/ForkPoolWorker-1] [1m[95m# Agent:[00m [1m[92mSpecialiste de la recherche[00m
[2025-05-07 07:12:52,506: WARNING/ForkPoolWorker-1] [95m## Task:[00m [92mAnalysez le matériel éducatif fourni par l'utilisateur : /tmp/tmp39amhs3z/sT0-Obedience-Checklist.pdf, et transformez-le en un contenu enrichi et structuré pour une utilisation ultérieure : 1. Extraire et organiser les sections principales, idées clés, et concepts importants du matériel d'étude. 2. Compléter les idées principales en effectuant des recherches supplémentaires (par ex., via des outils comme Serper) pour fournir des exemples, des analogies ou des données pertinentes. 3. Enrichissez le contenu extrait avec des exemples concrets, des applications pratiques et des informations supplémentaires grâce à des recherches complémentaires. 4. Générer un document final structuré et clair qui résume les idées clés tout en ajoutant des informations complémentaires. 5. Assurez-vous que la sortie soit adaptée pour une exploration approfondie et facilement réutilisable par d'autres agents ou utilisateurs.
[00m
[2025-05-07 07:12:52,508: WARNING/ForkPoolWorker-1] 🤖 Agent: Specialiste de la recherche

    Status: In Progress
└── 🧠 Thinking...
[92m07:12:52 - LiteLLM:INFO[0m: utils.py:3100 - 
LiteLLM completion() model= hf:mistralai/Mixtral-8x22B-Instruct-v0.1; provider = openai
[2025-05-07 07:12:52,532: INFO/ForkPoolWorker-1] 
LiteLLM completion() model= hf:mistralai/Mixtral-8x22B-Instruct-v0.1; provider = openai
[2025-05-07 07:12:54,982: INFO/ForkPoolWorker-1] HTTP Request: POST https://api.glhf.chat/v1/chat/completions "HTTP/1.1 200 OK"
[92m07:12:55 - LiteLLM:INFO[0m: utils.py:1177 - Wrapper: Completed Call, calling success_handler
[2025-05-07 07:12:55,010: INFO/ForkPoolWorker-1] Wrapper: Completed Call, calling success_handler
[92m07:12:55 - LiteLLM:INFO[0m: cost_calculator.py:636 - selected model name for cost calculation: openai/mistralai/Mixtral-8x22B-Instruct-v0.1
[2025-05-07 07:12:55,011: INFO/ForkPoolWorker-1] selected model name for cost calculation: openai/mistralai/Mixtral-8x22B-Instruct-v0.1
[92m07:12:55 - LiteLLM:INFO[0m: cost_calculator.py:636 - selected model name for cost calculation: openai/mistralai/Mixtral-8x22B-Instruct-v0.1
[2025-05-07 07:12:55,012: INFO/ForkPoolWorker-1] selected model name for cost calculation: openai/mistralai/Mixtral-8x22B-Instruct-v0.1
[92m07:12:55 - LiteLLM:INFO[0m: cost_calculator.py:636 - selected model name for cost calculation: hf:mistralai/Mixtral-8x22B-Instruct-v0.1
[2025-05-07 07:12:55,013: INFO/ForkPoolWorker-1] selected model name for cost calculation: hf:mistralai/Mixtral-8x22B-Instruct-v0.1
[92m07:12:55 - LiteLLM:INFO[0m: cost_calculator.py:636 - selected model name for cost calculation: openai/hf:mistralai/Mixtral-8x22B-Instruct-v0.1
[2025-05-07 07:12:55,013: INFO/ForkPoolWorker-1] selected model name for cost calculation: openai/hf:mistralai/Mixtral-8x22B-Instruct-v0.1
[2025-05-07 07:12:55,265: WARNING/ForkPoolWorker-1] 🤖 Agent: Specialiste de la recherche

    Status: In Progress
[92m07:12:55 - LiteLLM:INFO[0m: cost_calculator.py:636 - selected model name for cost calculation: openai/mistralai/Mixtral-8x22B-Instruct-v0.1
[2025-05-07 07:12:55,266: INFO/ForkPoolWorker-1] selected model name for cost calculation: openai/mistralai/Mixtral-8x22B-Instruct-v0.1
[92m07:12:55 - LiteLLM:INFO[0m: cost_calculator.py:636 - selected model name for cost calculation: hf:mistralai/Mixtral-8x22B-Instruct-v0.1
[2025-05-07 07:12:55,270: INFO/ForkPoolWorker-1] selected model name for cost calculation: hf:mistralai/Mixtral-8x22B-Instruct-v0.1
[2025-05-07 07:12:55,290: WARNING/ForkPoolWorker-1] 🤖 Agent: Specialiste de la recherche

    Status: In Progress
[2025-05-07 07:12:55,461: WARNING/ForkPoolWorker-1] 

[1m[95m# Agent:[00m [1m[92mSpecialiste de la recherche[00m
[2025-05-07 07:12:55,462: WARNING/ForkPoolWorker-1] [95m## Thought:[00m [92mI need to analyze the educational material provided by the user, extract the main sections and key ideas, and conduct additional research to enrich the content. I will start by using the PDF reading tool to read the content of the study material located at /tmp/tmp39amhs3z/sT0-Obedience-Checklist.pdf.[00m
[2025-05-07 07:12:55,462: WARNING/ForkPoolWorker-1] [95m## Using tool:[00m [92mPDF reading tool[00m
[2025-05-07 07:12:55,462: WARNING/ForkPoolWorker-1] [95m## Tool Input:[00m [92m
"{\"path\": \"/tmp/tmp39amhs3z/sT0-Obedience-Checklist.pdf\"}"[00m
[2025-05-07 07:12:55,462: WARNING/ForkPoolWorker-1] [95m## Tool Output:[00m [92m
Ok slave, you’ve listened to the brainwashing file and you’re eager to know what 
comes next. That’s great because eagerness creates ease and helps you take the 
right next steps. 
Here are some things you can do to serve & obey the Academy while you 
brainwash yourself in preparation for the next level of slave training.  
MOST IMPORTANT:
 
	 Listen	to	the	brainwashing	file	as	much	as	possible.	
  Keep listening, slave. Listen until the next level of slave training is made 
available to you, and then keep listening after that. The more you listen, the 
deeper you go.
ONE OFFS:
 	 Give	us	your	feedback	(Lesson	3	in	your	training.)
 	 Put	GsA	in	your	online	profiles	(Recon,	Grindr,	Scruff,	Growlr,	etc)
  you may put “slave is enrolled in training @GayslaveAcademy” to signal to 
Men that you are serious about being a good slave.
 	 Follow	us	on	Twitter	@gayslaveacademy .
  If you don’t have a Twitter account yet, make one. 
 	 Tweet	a	special	picture	of	yourself	and	tag	us	in	it.
  Let us show the world what a good boy you are: Tweet a picture with “Gay 
slave Academy” written on or near your body and tag  
@gayslaveacademy so we can retweet your pic.
ONGOING:
 
 Spread	the	word.	Tell	other	subs	(and	their	Masters)	about	GsA.
 
 Retweet	our	posts	on	Twitter!
THE SERVE & OBEY CHECKLIST
Move to the head of the class  
& prepare yourself for the next 
level of slave training…
© 2021 Gay slave Academy • www.gayslaveacademy.com[00m
[2025-05-07 07:12:55,463: WARNING/ForkPoolWorker-1] 🤖 Agent: Specialiste de la recherche

    Status: In Progress
└── 🧠 Thinking...
[92m07:12:55 - LiteLLM:INFO[0m: utils.py:3100 - 
LiteLLM completion() model= hf:mistralai/Mixtral-8x22B-Instruct-v0.1; provider = openai
[2025-05-07 07:12:55,464: INFO/ForkPoolWorker-1] 
LiteLLM completion() model= hf:mistralai/Mixtral-8x22B-Instruct-v0.1; provider = openai
[2025-05-07 07:12:56,262: INFO/ForkPoolWorker-1] HTTP Request: POST https://api.glhf.chat/v1/chat/completions "HTTP/1.1 400 Bad Request"
[2025-05-07 07:12:56,266: WARNING/ForkPoolWorker-1] LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.
[2025-05-07 07:12:56,267: WARNING/ForkPoolWorker-1] [1;31mProvider List: https://docs.litellm.ai/docs/providers[0m
[2025-05-07 07:12:56,633: WARNING/ForkPoolWorker-1] 🚀 Crew: crew
└── 📋 Task: d06e3dbe-1412-4822-a3bb-133b7215da38
       Status: Executing Task...
    └── 🤖 Agent: Specialiste de la recherche
        
            Status: In Progress
        └── ❌ LLM Failed
[2025-05-07 07:12:56,636: WARNING/ForkPoolWorker-1] ╭───────────────────────────────── LLM Error ──────────────────────────────────╮
│                                                                              │
│  ❌ LLM Call Failed                                                          │
│  Error: litellm.BadRequestError: OpenAIException - Error code: 400 -         │
│  {'error': 'Error from inference backend: 400 Input validation error'}       │
│                                                                              │
╰──────────────────────────────────────────────────────────────────────────────╯
[2025-05-07 07:12:56,636: ERROR/ForkPoolWorker-1] LiteLLM call failed: litellm.BadRequestError: OpenAIException - Error code: 400 - {'error': 'Error from inference backend: 400 Input validation error'}
[2025-05-07 07:12:56,638: WARNING/ForkPoolWorker-1] [31;1m🖇 AgentOps: Error in trace creation: litellm.BadRequestError: OpenAIException - Error code: 400 - {'error': 'Error from inference backend: 400 Input validation error'}[0m
[2025-05-07 07:12:56,716: WARNING/ForkPoolWorker-1] [91m Error during LLM call: litellm.BadRequestError: OpenAIException - Error code: 400 - {'error': 'Error from inference backend: 400 Input validation error'}[00m
[2025-05-07 07:12:56,717: WARNING/ForkPoolWorker-1] [91m An unknown error occurred. Please check the details below.[00m
[2025-05-07 07:12:56,718: WARNING/ForkPoolWorker-1] [91m Error details: litellm.BadRequestError: OpenAIException - Error code: 400 - {'error': 'Error from inference backend: 400 Input validation error'}[00m
[2025-05-07 07:12:56,721: WARNING/ForkPoolWorker-1] [31;1m🖇 AgentOps: Error in trace creation: litellm.BadRequestError: OpenAIException - Error code: 400 - {'error': 'Error from inference backend: 400 Input validation error'}[0m
[2025-05-07 07:12:56,798: WARNING/ForkPoolWorker-1] 🚀 Crew: crew
└── 📋 Task: d06e3dbe-1412-4822-a3bb-133b7215da38
       Assigned to: Specialiste de la recherche
    
       Status: ❌ Failed
    └── 🤖 Agent: Specialiste de la recherche
        
            Status: In Progress
        └── ❌ LLM Failed
[2025-05-07 07:12:56,803: WARNING/ForkPoolWorker-1] ╭──────────────────────────────── Task Failure ────────────────────────────────╮
│                                                                              │
│  Task Failed                                                                 │
│  Name: d06e3dbe-1412-4822-a3bb-133b7215da38                                  │
│  Agent: Specialiste de la recherche                                          │
│                                                                              │
│                                                                              │
│                                                                              │
╰──────────────────────────────────────────────────────────────────────────────╯
[2025-05-07 07:12:56,805: WARNING/ForkPoolWorker-1] [31;1m🖇 AgentOps: Error in trace creation: litellm.BadRequestError: OpenAIException - Error code: 400 - {'error': 'Error from inference backend: 400 Input validation error'}[0m
[2025-05-07 07:12:56,843: WARNING/ForkPoolWorker-1] ╭──────────────────────────────── Crew Failure ────────────────────────────────╮
│                                                                              │
│  Crew Execution Failed                                                       │
│  Name: crew                                                                  │
│  ID: 3eddbcf1-77c4-4060-8e9d-bb0cf6a7f9d5                                    │
│                                                                              │
│                                                                              │
╰──────────────────────────────────────────────────────────────────────────────╯
[2025-05-07 07:12:56,844: WARNING/ForkPoolWorker-1] [31;1m🖇 AgentOps: Error in trace creation: litellm.BadRequestError: OpenAIException - Error code: 400 - {'error': 'Error from inference backend: 400 Input validation error'}[0m
[2025-05-07 07:12:56,874: WARNING/ForkPoolWorker-1] 🖇 AgentOps: [34mSession Replay: https://app.agentops.ai/sessions?trace_id=97ef5a5a7215205ba6f5451ab9c4450e[0m
[2025-05-07 07:12:57,342: WARNING/ForkPoolWorker-1] [31;1m🖇 AgentOps: [agentops.InternalSpanProcessor] Error uploading logfile: Upload failed: 502[0m
[2025-05-07 07:12:57,344: WARNING/ForkPoolWorker-1] An error occurred: Error processing material: litellm.BadRequestError: OpenAIException - Error code: 400 - {'error': 'Error from inference backend: 400 Input validation error'}
[2025-05-07 07:12:57,817: WARNING/ForkPoolWorker-1] Notification sent successfully: 200
[2025-05-07 07:12:57,818: INFO/ForkPoolWorker-1] Task src.celery_app.process_file_task[2d3f3fec-eef4-47e7-a286-b2ef5a9250b5] succeeded in 7.5145976550011255s: {'error': 'Error processing material: litellm.BadRequestError: OpenAIException - Error code: 400 - {\'error\': \'Error from inference backend: 400 Input validation error\'}'}
